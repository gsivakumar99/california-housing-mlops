{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Name: MLOPS Group 63\n",
    "\n",
    "## Group Member Names:\n",
    "1.   Sivakumar G - 2023aa05486\n",
    "2.   Pabbisetty Jayakrishna - 2023aa05487\n",
    "3.   Ravi shankar S - 2023aa05488\n",
    "4.   Srivatsan V R - 2023aa05962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3: Model Experimentation and Packaging\n",
    "\n",
    "**Objective**: Train a machine learning model, perform hyperparameter tuning, and package the model for deployment.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "#### 1. Hyperparameter Tuning:\n",
    "- Use a library like Optuna or Scikit-learn’s GridSearchCV to perform hyperparameter tuning on a chosen model.\n",
    "- Document the tuning process and the best parameters found.\n",
    "\n",
    "#### 2. Model Packaging:\n",
    "- Package the best-performing model using tools like Docker and Flask.\n",
    "- Create a Dockerfile and a simple Flask application to serve the model.\n",
    "\n",
    "### Deliverables:\n",
    "- A report on hyperparameter tuning results.\n",
    "- A Dockerfile and Flask application code.\n",
    "- Screenshots of the model running in a Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook captures the following tasks:\n",
    "\n",
    "#### 1. Hyperparameter Tuning:\n",
    "- The notebook uses libraries like Optuna or Scikit-learn’s GridSearchCV to perform hyperparameter tuning on the selected model.\n",
    "- It documents the entire tuning process, including the hyperparameters tested and the best-performing parameters discovered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: Libraries needed for Optuna, scikit-learn, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import optuna\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading: Fetch and split the California Housing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function: \n",
    "The function Optuna will optimize. This function will suggest different values for hyperparameters of the RandomForest model and evaluate the model using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 30, step=5)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20, step=2)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\")\n",
    "    avg_mse = -np.mean(scores)\n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Optimization: \n",
    "This part runs the optimization process, trying 50 different sets of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-26 23:50:16,499] A new study created in memory with name: no-name-3cfeb86f-6c24-4f90-b19c-976c0748f6b0\n",
      "[I 2025-01-26 23:51:06,220] Trial 0 finished with value: 0.2799784137895807 and parameters: {'n_estimators': 350, 'max_depth': 25, 'min_samples_split': 14, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2799784137895807.\n",
      "[I 2025-01-26 23:52:28,019] Trial 1 finished with value: 0.27457571283102794 and parameters: {'n_estimators': 400, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:53:12,211] Trial 2 finished with value: 0.2785342581544273 and parameters: {'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:53:40,674] Trial 3 finished with value: 0.44420870693335307 and parameters: {'n_estimators': 350, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:54:27,634] Trial 4 finished with value: 0.28578177814243194 and parameters: {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:55:40,221] Trial 5 finished with value: 0.2747816747063055 and parameters: {'n_estimators': 350, 'max_depth': 15, 'min_samples_split': 18, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:56:25,845] Trial 6 finished with value: 0.29287349128411705 and parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:57:03,264] Trial 7 finished with value: 0.44479343715089775 and parameters: {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:57:30,264] Trial 8 finished with value: 0.2793117434904573 and parameters: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 12, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.27457571283102794.\n",
      "[I 2025-01-26 23:58:36,040] Trial 9 finished with value: 0.2778982033241502 and parameters: {'n_estimators': 400, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.27457571283102794.\n"
     ]
    }
   ],
   "source": [
    "# Create Optuna study and perform optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    "Prints the best hyperparameters and the corresponding MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'n_estimators': 400, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 6}\n",
      "\n",
      "Best MSE: 0.27457571283102794\n"
     ]
    }
   ],
   "source": [
    "# Display best parameters and best score\n",
    "print(\"Best Parameters:\")\n",
    "print(study.best_params)\n",
    "print(\"\\nBest MSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test MSE: 0.2649736667996871\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model with the best parameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"\\nTest MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test MSE: 0.2649736667996871\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model with the best parameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"\\nTest MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the best performacing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'best_rf_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n",
    "print(\"Model saved as 'best_rf_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary of Hyperparameter Tuning:\n",
    "\n",
    "The hyperparameter tuning process involved using Optuna to optimize the parameters of a Random Forest model. The following parameters were tuned during the process:\n",
    "\n",
    "- **n_estimators**: Number of trees in the forest\n",
    "- **max_depth**: Maximum depth of the trees\n",
    "- **min_samples_split**: Minimum number of samples required to split an internal node\n",
    "- **min_samples_leaf**: Minimum number of samples required to be at a leaf node\n",
    "\n",
    "#### Tuning Process:\n",
    "The tuning was performed using the Optuna library, which carried out multiple trials to find the best combination of hyperparameters. Each trial evaluated the model's performance based on a specific set of parameters, and the goal was to minimize the validation error (represented by the value).\n",
    "\n",
    "The trials ran with the following results:\n",
    "\n",
    "- **Trial 0**: `{'n_estimators': 350, 'max_depth': 25, 'min_samples_split': 14, 'min_samples_leaf': 8}` resulted in a validation error of `0.2799784137895807`.\n",
    "- **Trial 1**: `{'n_estimators': 400, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 6}` achieved the best result so far with a validation error of `0.27457571283102794`.\n",
    "- **Trial 2**: `{'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 7}` resulted in `0.2785342581544273`.\n",
    "- **Trial 3**: `{'n_estimators': 350, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2}` resulted in a much higher validation error of `0.44420870693335307`.\n",
    "- **Trial 4**: `{'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 10}` gave a validation error of `0.28578177814243194`.\n",
    "- **Trial 5**: `{'n_estimators': 350, 'max_depth': 15, 'min_samples_split': 18, 'min_samples_leaf': 3}` gave a validation error of `0.2747816747063055`, slightly worse than Trial 1.\n",
    "- **Trial 6**: `{'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 5}` resulted in a validation error of `0.29287349128411705`.\n",
    "- **Trial 7**: `{'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 1}` produced a much higher validation error of `0.44479343715089775`.\n",
    "- **Trial 8**: `{'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 12, 'min_samples_leaf': 7}` resulted in `0.2793117434904573`.\n",
    "- **Trial 9**: `{'n_estimators': 400, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 7}` resulted in `0.2778982033241502`.\n",
    "\n",
    "#### Best Hyperparameters:\n",
    "The best combination of hyperparameters found during the tuning process was:\n",
    "- **n_estimators**: 400\n",
    "- **max_depth**: 25\n",
    "- **min_samples_split**: 10\n",
    "- **min_samples_leaf**: 6\n",
    "\n",
    "This combination resulted in the lowest validation error of **0.27457571283102794** in Trial 1, making it the optimal choice for the model.\n",
    "\n",
    "### Conclusion:\n",
    "The hyperparameter tuning process successfully identified the best set of parameters, improving the model's performance. These optimal parameters will be used in the final model for deployment and further evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
